{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d8f2b60",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd958dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import joblib\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Models\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import Ridge, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor\n",
    "\n",
    "# MLflow\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# Project paths\n",
    "PROJECT_ROOT = Path('/workspaces/Recommendation-system')\n",
    "DATA_PROCESSED = PROJECT_ROOT / 'data' / 'processed'\n",
    "MODELS_DIR = PROJECT_ROOT / 'models'\n",
    "REPORTS_FIGURES = PROJECT_ROOT / 'reports' / 'figures'\n",
    "MLRUNS_DIR = PROJECT_ROOT / 'mlruns'\n",
    "\n",
    "# Create directories\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Constants\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05caf1c",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395cf1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load consolidated dataset\n",
    "df = pd.read_csv(DATA_PROCESSED / 'consolidated.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5183462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "NUMERIC_FEATURES = ['rainfall_mm', 'pesticides_tonnes', 'avg_temp']\n",
    "CATEGORICAL_FEATURES = ['crop', 'country']\n",
    "TARGET = 'yield'\n",
    "\n",
    "print(f\"Numeric features: {NUMERIC_FEATURES}\")\n",
    "print(f\"Categorical features: {CATEGORICAL_FEATURES}\")\n",
    "print(f\"Target: {TARGET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d075d8",
   "metadata": {},
   "source": [
    "## 3. Principal Component Analysis (PCA)\n",
    "\n",
    "PCA is used to identify key variables driving variance in the context space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27ee585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for PCA (numeric features only, standardized)\n",
    "X_numeric = df[NUMERIC_FEATURES].copy()\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_numeric)\n",
    "\n",
    "# Fit PCA\n",
    "pca = PCA()\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "print(\"PCA Results:\")\n",
    "print(f\"Number of components: {pca.n_components_}\")\n",
    "print(f\"\\nExplained variance ratio:\")\n",
    "for i, var in enumerate(pca.explained_variance_ratio_):\n",
    "    print(f\"  PC{i+1}: {var:.4f} ({var*100:.2f}%)\")\n",
    "print(f\"\\nCumulative explained variance: {pca.explained_variance_ratio_.cumsum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4921625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot explained variance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scree plot\n",
    "components = range(1, len(pca.explained_variance_ratio_) + 1)\n",
    "axes[0].bar(components, pca.explained_variance_ratio_, alpha=0.7, label='Individual')\n",
    "axes[0].plot(components, pca.explained_variance_ratio_.cumsum(), 'ro-', label='Cumulative')\n",
    "axes[0].axhline(y=0.95, color='g', linestyle='--', label='95% threshold')\n",
    "axes[0].set_xlabel('Principal Component')\n",
    "axes[0].set_ylabel('Explained Variance Ratio')\n",
    "axes[0].set_title('PCA Explained Variance')\n",
    "axes[0].legend()\n",
    "axes[0].set_xticks(components)\n",
    "\n",
    "# Loadings heatmap\n",
    "loadings = pd.DataFrame(\n",
    "    pca.components_.T,\n",
    "    columns=[f'PC{i+1}' for i in range(pca.n_components_)],\n",
    "    index=NUMERIC_FEATURES\n",
    ")\n",
    "sns.heatmap(loadings, annot=True, cmap='RdBu_r', center=0, ax=axes[1], fmt='.3f')\n",
    "axes[1].set_title('PCA Loadings Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_FIGURES / 'pca_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a732bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA Interpretation\n",
    "print(\"=\" * 60)\n",
    "print(\"PCA INTERPRETATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nLoadings Matrix:\")\n",
    "print(loadings.round(3))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"KEY FINDINGS:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Find dominant features for each component\n",
    "for i in range(min(3, pca.n_components_)):\n",
    "    pc_loadings = loadings.iloc[:, i].abs().sort_values(ascending=False)\n",
    "    dominant = pc_loadings.index[0]\n",
    "    print(f\"\\nPC{i+1} ({pca.explained_variance_ratio_[i]*100:.1f}% variance):\")\n",
    "    print(f\"  Dominated by: {dominant}\")\n",
    "    for feat in NUMERIC_FEATURES:\n",
    "        loading = loadings.loc[feat, f'PC{i+1}']\n",
    "        direction = 'positive' if loading > 0 else 'negative'\n",
    "        print(f\"  - {feat}: {loading:.3f} ({direction})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25987636",
   "metadata": {},
   "source": [
    "## 4. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24efe83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = df[NUMERIC_FEATURES + CATEGORICAL_FEATURES]\n",
    "y = df[TARGET]\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b1095b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-based split: use earlier years for training, later for testing\n",
    "# First, let's check year distribution\n",
    "print(f\"Year range: {df['year'].min()} - {df['year'].max()}\")\n",
    "\n",
    "# Split at ~80% of data (by year)\n",
    "year_cutoff = df['year'].quantile(0.8)\n",
    "print(f\"Year cutoff for train/test split: {year_cutoff}\")\n",
    "\n",
    "# Create train/test masks\n",
    "train_mask = df['year'] <= year_cutoff\n",
    "test_mask = df['year'] > year_cutoff\n",
    "\n",
    "X_train = X[train_mask]\n",
    "X_test = X[test_mask]\n",
    "y_train = y[train_mask]\n",
    "y_test = y[test_mask]\n",
    "\n",
    "print(f\"\\nTraining set: {len(X_train)} samples ({100*len(X_train)/len(X):.1f}%)\")\n",
    "print(f\"Test set: {len(X_test)} samples ({100*len(X_test)/len(X):.1f}%)\")\n",
    "print(f\"Training years: {df[train_mask]['year'].min()} - {df[train_mask]['year'].max()}\")\n",
    "print(f\"Test years: {df[test_mask]['year'].min()} - {df[test_mask]['year'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296f5bb3",
   "metadata": {},
   "source": [
    "## 5. Create Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf57b155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessor\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, NUMERIC_FEATURES),\n",
    "        ('cat', categorical_transformer, CATEGORICAL_FEATURES)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Preprocessor created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ac6d46",
   "metadata": {},
   "source": [
    "## 6. MLflow Setup and Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573eaea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup MLflow\n",
    "mlflow.set_tracking_uri(str(MLRUNS_DIR))\n",
    "mlflow.set_experiment(\"crop_yield_prediction\")\n",
    "\n",
    "print(f\"MLflow tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"Experiment: crop_yield_prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e87d5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Train and evaluate a model, returning metrics.\"\"\"\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Training metrics\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    \n",
    "    # Test metrics\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    return {\n",
    "        'train_rmse': train_rmse,\n",
    "        'train_mae': train_mae,\n",
    "        'train_r2': train_r2,\n",
    "        'test_rmse': test_rmse,\n",
    "        'test_mae': test_mae,\n",
    "        'test_r2': test_r2,\n",
    "        'y_test_pred': y_test_pred\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804c5d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to train\n",
    "models = {\n",
    "    'Baseline (Mean)': DummyRegressor(strategy='mean'),\n",
    "    'Ridge Regression': Ridge(alpha=1.0, random_state=RANDOM_STATE),\n",
    "    'ElasticNet': ElasticNet(alpha=1.0, l1_ratio=0.5, random_state=RANDOM_STATE),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=15, random_state=RANDOM_STATE, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, max_depth=5, learning_rate=0.1, random_state=RANDOM_STATE),\n",
    "    'HistGradientBoosting': HistGradientBoostingRegressor(max_iter=100, max_depth=10, learning_rate=0.1, random_state=RANDOM_STATE)\n",
    "}\n",
    "\n",
    "print(f\"Training {len(models)} models...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0c8bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models with MLflow tracking\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining: {name}\")\n",
    "    \n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    with mlflow.start_run(run_name=name):\n",
    "        # Log parameters\n",
    "        mlflow.log_param('model_type', name)\n",
    "        mlflow.log_param('numeric_features', NUMERIC_FEATURES)\n",
    "        mlflow.log_param('categorical_features', CATEGORICAL_FEATURES)\n",
    "        \n",
    "        # Log model-specific parameters\n",
    "        if hasattr(model, 'get_params'):\n",
    "            params = model.get_params()\n",
    "            for key, value in params.items():\n",
    "                if value is not None and not callable(value):\n",
    "                    mlflow.log_param(f'model_{key}', value)\n",
    "        \n",
    "        # Train and evaluate\n",
    "        metrics = evaluate_model(pipeline, X_train, X_test, y_train, y_test)\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric('train_rmse', metrics['train_rmse'])\n",
    "        mlflow.log_metric('train_mae', metrics['train_mae'])\n",
    "        mlflow.log_metric('train_r2', metrics['train_r2'])\n",
    "        mlflow.log_metric('test_rmse', metrics['test_rmse'])\n",
    "        mlflow.log_metric('test_mae', metrics['test_mae'])\n",
    "        mlflow.log_metric('test_r2', metrics['test_r2'])\n",
    "        \n",
    "        # Log model\n",
    "        mlflow.sklearn.log_model(pipeline, 'model')\n",
    "        \n",
    "        # Store results\n",
    "        results[name] = {\n",
    "            'pipeline': pipeline,\n",
    "            'metrics': metrics\n",
    "        }\n",
    "        \n",
    "        print(f\"  Test RMSE: {metrics['test_rmse']:.2f}\")\n",
    "        print(f\"  Test MAE: {metrics['test_mae']:.2f}\")\n",
    "        print(f\"  Test R²: {metrics['test_r2']:.4f}\")\n",
    "\n",
    "print(\"\\n✅ All models trained and logged to MLflow!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954aed14",
   "metadata": {},
   "source": [
    "## 7. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dcdc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_data = []\n",
    "for name, result in results.items():\n",
    "    metrics = result['metrics']\n",
    "    comparison_data.append({\n",
    "        'Model': name,\n",
    "        'Train RMSE': metrics['train_rmse'],\n",
    "        'Test RMSE': metrics['test_rmse'],\n",
    "        'Train MAE': metrics['train_mae'],\n",
    "        'Test MAE': metrics['test_mae'],\n",
    "        'Train R²': metrics['train_r2'],\n",
    "        'Test R²': metrics['test_r2']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('Test R²', ascending=False)\n",
    "\n",
    "print(\"Model Comparison (sorted by Test R²):\")\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0396a891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# RMSE comparison\n",
    "x = range(len(comparison_df))\n",
    "width = 0.35\n",
    "axes[0].bar([i - width/2 for i in x], comparison_df['Train RMSE'], width, label='Train', alpha=0.8)\n",
    "axes[0].bar([i + width/2 for i in x], comparison_df['Test RMSE'], width, label='Test', alpha=0.8)\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(comparison_df['Model'], rotation=45, ha='right')\n",
    "axes[0].set_ylabel('RMSE')\n",
    "axes[0].set_title('RMSE Comparison')\n",
    "axes[0].legend()\n",
    "\n",
    "# MAE comparison\n",
    "axes[1].bar([i - width/2 for i in x], comparison_df['Train MAE'], width, label='Train', alpha=0.8)\n",
    "axes[1].bar([i + width/2 for i in x], comparison_df['Test MAE'], width, label='Test', alpha=0.8)\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(comparison_df['Model'], rotation=45, ha='right')\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].set_title('MAE Comparison')\n",
    "axes[1].legend()\n",
    "\n",
    "# R² comparison\n",
    "axes[2].bar([i - width/2 for i in x], comparison_df['Train R²'], width, label='Train', alpha=0.8)\n",
    "axes[2].bar([i + width/2 for i in x], comparison_df['Test R²'], width, label='Test', alpha=0.8)\n",
    "axes[2].set_xticks(x)\n",
    "axes[2].set_xticklabels(comparison_df['Model'], rotation=45, ha='right')\n",
    "axes[2].set_ylabel('R²')\n",
    "axes[2].set_title('R² Comparison')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_FIGURES / 'model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abc8b2f",
   "metadata": {},
   "source": [
    "## 8. Select Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304c43a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model based on test R²\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "best_model = results[best_model_name]['pipeline']\n",
    "best_metrics = results[best_model_name]['metrics']\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BEST MODEL SELECTED\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nModel: {best_model_name}\")\n",
    "print(f\"\\nTest Metrics:\")\n",
    "print(f\"  RMSE: {best_metrics['test_rmse']:.2f}\")\n",
    "print(f\"  MAE: {best_metrics['test_mae']:.2f}\")\n",
    "print(f\"  R²: {best_metrics['test_r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ca32b0",
   "metadata": {},
   "source": [
    "## 9. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828e0a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names after preprocessing\n",
    "def get_feature_names(preprocessor, numeric_features, categorical_features):\n",
    "    \"\"\"Extract feature names from fitted preprocessor.\"\"\"\n",
    "    feature_names = list(numeric_features)  # Numeric features first\n",
    "    \n",
    "    # Get one-hot encoded feature names\n",
    "    cat_encoder = preprocessor.named_transformers_['cat'].named_steps['encoder']\n",
    "    cat_feature_names = cat_encoder.get_feature_names_out(categorical_features)\n",
    "    feature_names.extend(cat_feature_names)\n",
    "    \n",
    "    return feature_names\n",
    "\n",
    "# Get feature names\n",
    "fitted_preprocessor = best_model.named_steps['preprocessor']\n",
    "feature_names = get_feature_names(fitted_preprocessor, NUMERIC_FEATURES, CATEGORICAL_FEATURES)\n",
    "print(f\"Total features after encoding: {len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf6abb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importance (for tree-based models)\n",
    "model_estimator = best_model.named_steps['model']\n",
    "\n",
    "if hasattr(model_estimator, 'feature_importances_'):\n",
    "    importances = model_estimator.feature_importances_\n",
    "    \n",
    "    # Create DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names[:len(importances)],\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Plot top 20 features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_n = min(20, len(importance_df))\n",
    "    plt.barh(importance_df['feature'][:top_n][::-1], importance_df['importance'][:top_n][::-1])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title(f'Top {top_n} Feature Importances ({best_model_name})')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(REPORTS_FIGURES / 'feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nTop 10 Features:\")\n",
    "    print(importance_df.head(10))\n",
    "else:\n",
    "    print(f\"{best_model_name} does not have feature_importances_ attribute\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead6090f",
   "metadata": {},
   "source": [
    "## 10. Prediction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfabda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual vs Predicted plot\n",
    "y_test_pred = best_metrics['y_test_pred']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scatter plot\n",
    "axes[0].scatter(y_test, y_test_pred, alpha=0.3, s=10)\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='Perfect prediction')\n",
    "axes[0].set_xlabel('Actual Yield (hg/ha)')\n",
    "axes[0].set_ylabel('Predicted Yield (hg/ha)')\n",
    "axes[0].set_title(f'Actual vs Predicted ({best_model_name})')\n",
    "axes[0].legend()\n",
    "\n",
    "# Residuals distribution\n",
    "residuals = y_test - y_test_pred\n",
    "axes[1].hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Residual (Actual - Predicted)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Residuals Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_FIGURES / 'prediction_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nResiduals Statistics:\")\n",
    "print(f\"  Mean: {residuals.mean():.2f}\")\n",
    "print(f\"  Std: {residuals.std():.2f}\")\n",
    "print(f\"  Min: {residuals.min():.2f}\")\n",
    "print(f\"  Max: {residuals.max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150f5b91",
   "metadata": {},
   "source": [
    "## 11. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52836ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model pipeline\n",
    "model_path = MODELS_DIR / 'model_pipeline.joblib'\n",
    "joblib.dump(best_model, model_path)\n",
    "\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "print(f\"File size: {model_path.stat().st_size / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257d562c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model metadata\n",
    "metadata = {\n",
    "    'model_name': best_model_name,\n",
    "    'numeric_features': NUMERIC_FEATURES,\n",
    "    'categorical_features': CATEGORICAL_FEATURES,\n",
    "    'target': TARGET,\n",
    "    'test_rmse': best_metrics['test_rmse'],\n",
    "    'test_mae': best_metrics['test_mae'],\n",
    "    'test_r2': best_metrics['test_r2'],\n",
    "    'training_samples': len(X_train),\n",
    "    'test_samples': len(X_test),\n",
    "    'supported_crops': df['crop'].unique().tolist(),\n",
    "    'supported_countries': df['country'].unique().tolist()\n",
    "}\n",
    "\n",
    "# Save as JSON\n",
    "import json\n",
    "metadata_path = MODELS_DIR / 'model_metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"Metadata saved to: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ab7a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify model can be loaded and used\n",
    "loaded_model = joblib.load(model_path)\n",
    "\n",
    "# Test prediction\n",
    "test_input = pd.DataFrame([{\n",
    "    'rainfall_mm': 1000,\n",
    "    'pesticides_tonnes': 5000,\n",
    "    'avg_temp': 20,\n",
    "    'crop': 'Wheat',\n",
    "    'country': 'India'\n",
    "}])\n",
    "\n",
    "prediction = loaded_model.predict(test_input)\n",
    "print(f\"\\nTest prediction for Wheat in India:\")\n",
    "print(f\"  Input: rainfall=1000mm, pesticides=5000t, temp=20°C\")\n",
    "print(f\"  Predicted yield: {prediction[0]:.0f} hg/ha\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a5acc3",
   "metadata": {},
   "source": [
    "## 12. MLflow Summary\n",
    "\n",
    "To view MLflow UI, run in terminal:\n",
    "```bash\n",
    "mlflow ui --backend-store-uri /workspaces/Recommendation-system/mlruns\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce33e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nExperiment: crop_yield_prediction\")\n",
    "print(f\"MLflow tracking directory: {MLRUNS_DIR}\")\n",
    "print(f\"\\nModels trained: {len(models)}\")\n",
    "for name in models.keys():\n",
    "    print(f\"  - {name}\")\n",
    "print(f\"\\nBest model: {best_model_name}\")\n",
    "print(f\"Best test R²: {best_metrics['test_r2']:.4f}\")\n",
    "print(f\"\\nModel artifact: {model_path}\")\n",
    "print(f\"\\n✅ Modeling complete! Ready for API integration.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
